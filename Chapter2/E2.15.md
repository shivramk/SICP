Eva lu ator isn't exactly right (although she is right to a certain extent).
`par2` is a better program than `par1` for computing parallel resistances.
The reason is that `par2` keeps error tolerances down to minimum at each state
of computation. In `par1` the error tolerances get compounded.

We know from exercise 2.13 that given two intervals $i_1$ and $i_2$ in the center
percent form the percentage tolerance of the result is the sum of the
percentage tolerances of the factors. The same goes for division as it is
nothing but inverse multiplication.  The resulting tolerance of adding two
intervals with same tolerance is same as the original tolerance of either one
of the arguments.  In other words:

1. for $result = i_1 \cdot i_2$,  $t_{result} = t_1 + t_2$
2. for $result = \frac{i_1}{i_2}$,  $t_{result} = t_1 + t_2$
3. for $result = i_1 + i_2$,  $t_{result} = t\ (Assuming\ t_1=t_2=t)$

For the sake of simplification assume that both $R1$ and $R2$ have the same
tolerance $t$. We also assume that $t$ is small in order to simplify the analysis.

Now let us analyze both `par1` and `par2`

In par1 the steps are

1. $x = r_1 \cdot r_2$,      $t_x = t + t = 2t$
2. $y = r_1 + r_2$,      $t_y = t$
3. $result = \frac{x}{y}$,     $t_{result} = t_x + t_y = 3t$

The final tolerance for `par1` is roughly $3t$

In `par2` the steps are

1. $one = (1, 1)$,    $t_{one} = 0$
2. $x = \frac{one}{r_1}$,      $t_x = t_{one} + t = t$
3. $y = \frac{one}{r_2}$,      $t_y = t_{one} + t = t$
4. $z = x + y$,      $t_z = t_x\ or\ t_y = t$
5. $result = \frac{one}{z}$,  $t_{result} = t_{one} + t_z = t$

The final tolerance for `par2` is just $t$ (roughly)

By carefully factoring the expressions to keep tolerances to a minimum at each
stage `par2` produces a tighter error bound.
